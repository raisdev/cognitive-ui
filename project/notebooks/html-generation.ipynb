{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13275343-af40-4cce-be4a-972dbdc26947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e5ace5-6efe-4916-91c2-1525ede2e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ee4d7-e202-4cca-8e70-5a566b52a407",
   "metadata": {},
   "source": [
    "## LangChain Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95f23e3-b812-4411-9aac-1a5f7d246333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# https://python.langchain.com/docs/modules/memory/adding_memory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f0489-4119-447c-a117-f6e5f7e638cc",
   "metadata": {},
   "source": [
    "## Setup and Initialize Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7437077-8634-47da-a0c3-5731ee129bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_v1_template = \"\"\"You are an intelligent Cognitive UI agent that is having a conversation with an user.\n",
    "Your task is to have a conversation and respond appropriately to the user. You have the ability to generate UI components \n",
    "in the form of HTML code and HTML is the output format that the user prefers when needed.\n",
    "Before you output any HTML, you need to confirm that the HTML is valid, stylized with CSS or other frontend frameworks, attached with \n",
    "functional JS scripts and most importantly, it must satisfy the user's requirements. \n",
    "Valid HTML must follow the standard HTML file format containing the keyword 'DOCTYPE'.\n",
    "\n",
    "If the user provides feedback to the generated HTML, you will incorporate their feedback\n",
    "and make new attempts until they're satisfied. In order to generate HTML, you may need content from the user and it is okay to \n",
    "collect information from the user based on the type of UI component they need. \n",
    "\n",
    "Once you have enough information from the user, you need to output HTML code; you should\n",
    "also do this if the user is requesting for HTML. However, if you need more information, you should continue asking for information.\n",
    "If you have already generated HTML and the user provides feedback and suggest changes, please respond with the updated HTML.\n",
    "\n",
    "DO NOT tell the user that you will show them the UI component/code without actually including it in your response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a8351e5-3548-4ac2-8f71-6597cdf38d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chat(system_template, verbose_mode=False):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=system_template\n",
    "            ),  # The persistent system prompt\n",
    "            MessagesPlaceholder(\n",
    "                variable_name=\"chat_history\"\n",
    "            ),  # Where the memory will be stored.\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"{human_input}\"\n",
    "            ),  # Where the human input will injected\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(temperature = 0)\n",
    "    mem = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    \n",
    "    chat_llm_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=verbose_mode,\n",
    "        memory=mem,\n",
    "    )\n",
    "    return chat_llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d10182-4a70-47ed-8527-72b714d87019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chat_summary_mem(system_template, verbose_mode=False):\n",
    "    '''\n",
    "    creates a chat model that summarizes the current chat history to work with finite memory.\n",
    "    '''\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=system_template\n",
    "            ),  # The persistent system prompt\n",
    "            MessagesPlaceholder(\n",
    "                variable_name=\"chat_history\"\n",
    "            ),  # Where the memory will be stored.\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"{human_input}\"\n",
    "            ),  # Where the human input will injected\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(temperature = 0)\n",
    "    # mem = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    mem = ConversationSummaryMemory(memory_key=\"chat_history\", llm=ChatOpenAI(temperature = 0), return_messages=True)\n",
    "    \n",
    "    chat_llm_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=verbose_mode,\n",
    "        memory=mem,\n",
    "    )\n",
    "    return chat_llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a562db81-7b7c-45a7-aa0f-4c10c4bb5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_pattern = re.compile(r'<!DOCTYPE html>.*?</html>', re.DOTALL)\n",
    "def interact(model, input):\n",
    "    o = model.predict(human_input=input)\n",
    "    html_match = html_pattern.search(o)\n",
    "    s = False\n",
    "    if html_match:\n",
    "        s = True\n",
    "        display(HTML(html_match.group(0)))\n",
    "    if s:\n",
    "        o = re.sub(r\"```html.*?```\", \"\", o, flags=re.DOTALL)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33674601-3f65-4bc1-9edf-9e8346435a2f",
   "metadata": {},
   "source": [
    "## Testing \"Unbounded\" Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "999a7633-cfbc-460b-93f9-6e1cc402f326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_chat(system_v1_template, verbose_mode=False)\n",
    "interact(model, \"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0d2ecdc-6ffe-48f3-87d7-aeb29479ce3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great! I can help you with that. Could you please provide me with some details about the form you would like to have on your website? For example, what fields should the form include and any specific styling preferences you have in mind?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(model, \n",
    "\"\"\"I want a form for my website\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d14b2b-9757-41b1-97f1-afbfbb18b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(sum_model, \"\"\"The form needs to be resizable and the submit button should take up the entire space on its row\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ec78c-eecf-4576-bf85-baa6066112c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(sum_model, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
